{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local) created by __init__ at <ipython-input-2-d8d32de5396b>:23 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-d8d32de5396b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.cores.max\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\THeo\\Documents\\Kuliah\\Bigdata\\spark-2.1.0-bin-hadoop2.7/python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32mC:\\Users\\THeo\\Documents\\Kuliah\\Bigdata\\spark-2.1.0-bin-hadoop2.7/python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    270\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 272\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local) created by __init__ at <ipython-input-2-d8d32de5396b>:23 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = \"C:\\\\Users\\\\THeo\\\\Documents\\\\Kuliah\\\\Bigdata\\\\spark-2.1.0-bin-hadoop2.7\"\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "os.environ['HADOOP_HOME'] = spark_path\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.10.4-src.zip\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.cores.max\", \"4\")\n",
    "\n",
    "sc = SparkContext(\"local\", conf=conf)\n",
    "print (sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014-04-01,19805,1,JFK,LAX,0854,-6.00,1217,2.00,355.00,2475.00',\n",
       " '2014-04-01,19805,2,LAX,JFK,0944,14.00,1736,-29.00,269.00,2475.00',\n",
       " '2014-04-01,19805,3,JFK,LAX,1224,-6.00,1614,39.00,371.00,2475.00',\n",
       " '2014-04-01,19805,4,LAX,JFK,1240,25.00,2028,-27.00,264.00,2475.00',\n",
       " '2014-04-01,19805,5,DFW,HNL,1300,-5.00,1650,15.00,510.00,3784.00',\n",
       " '2014-04-01,19805,6,OGG,DFW,1901,126.00,0640,95.00,385.00,3711.00',\n",
       " '2014-04-01,19805,7,DFW,OGG,1410,125.00,1743,138.00,497.00,3711.00',\n",
       " '2014-04-01,19805,8,HNL,DFW,1659,4.00,0458,-22.00,398.00,3784.00',\n",
       " '2014-04-01,19805,9,JFK,LAX,0648,-7.00,1029,19.00,365.00,2475.00',\n",
       " '2014-04-01,19805,10,LAX,JFK,2156,21.00,0556,1.00,265.00,2475.00']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines=sc.textFile('C:\\\\Users\\\\THeo\\\\Documents\\\\Kuliah\\\\Bigdata\\\\dataset\\\\airlines.csv')\n",
    "flights=sc.textFile('C:\\\\Users\\\\THeo\\\\Documents\\\\Kuliah\\\\Bigdata\\\\dataset\\\\flights.csv')\n",
    "airports=sc.textFile('C:\\\\Users\\\\THeo\\\\Documents\\\\Kuliah\\\\Bigdata\\\\dataset\\\\airports.csv')\n",
    "\n",
    "flights.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from builtins import property as _property, tuple as _tuple\n",
      "from operator import itemgetter as _itemgetter\n",
      "from collections import OrderedDict\n",
      "\n",
      "class Flight(tuple):\n",
      "    'Flight(date, airline, flightnum, origin, dest, dep, dep_delay, arv, arv_delay, airtime, distance)'\n",
      "\n",
      "    __slots__ = ()\n",
      "\n",
      "    _fields = ('date', 'airline', 'flightnum', 'origin', 'dest', 'dep', 'dep_delay', 'arv', 'arv_delay', 'airtime', 'distance')\n",
      "\n",
      "    def __new__(_cls, date, airline, flightnum, origin, dest, dep, dep_delay, arv, arv_delay, airtime, distance):\n",
      "        'Create new instance of Flight(date, airline, flightnum, origin, dest, dep, dep_delay, arv, arv_delay, airtime, distance)'\n",
      "        return _tuple.__new__(_cls, (date, airline, flightnum, origin, dest, dep, dep_delay, arv, arv_delay, airtime, distance))\n",
      "\n",
      "    @classmethod\n",
      "    def _make(cls, iterable, new=tuple.__new__, len=len):\n",
      "        'Make a new Flight object from a sequence or iterable'\n",
      "        result = new(cls, iterable)\n",
      "        if len(result) != 11:\n",
      "            raise TypeError('Expected 11 arguments, got %d' % len(result))\n",
      "        return result\n",
      "\n",
      "    def _replace(_self, **kwds):\n",
      "        'Return a new Flight object replacing specified fields with new values'\n",
      "        result = _self._make(map(kwds.pop, ('date', 'airline', 'flightnum', 'origin', 'dest', 'dep', 'dep_delay', 'arv', 'arv_delay', 'airtime', 'distance'), _self))\n",
      "        if kwds:\n",
      "            raise ValueError('Got unexpected field names: %r' % list(kwds))\n",
      "        return result\n",
      "\n",
      "    def __repr__(self):\n",
      "        'Return a nicely formatted representation string'\n",
      "        return self.__class__.__name__ + '(date=%r, airline=%r, flightnum=%r, origin=%r, dest=%r, dep=%r, dep_delay=%r, arv=%r, arv_delay=%r, airtime=%r, distance=%r)' % self\n",
      "\n",
      "    def _asdict(self):\n",
      "        'Return a new OrderedDict which maps field names to their values.'\n",
      "        return OrderedDict(zip(self._fields, self))\n",
      "\n",
      "    def __getnewargs__(self):\n",
      "        'Return self as a plain tuple.  Used by copy and pickle.'\n",
      "        return tuple(self)\n",
      "\n",
      "    date = _property(_itemgetter(0), doc='Alias for field number 0')\n",
      "\n",
      "    airline = _property(_itemgetter(1), doc='Alias for field number 1')\n",
      "\n",
      "    flightnum = _property(_itemgetter(2), doc='Alias for field number 2')\n",
      "\n",
      "    origin = _property(_itemgetter(3), doc='Alias for field number 3')\n",
      "\n",
      "    dest = _property(_itemgetter(4), doc='Alias for field number 4')\n",
      "\n",
      "    dep = _property(_itemgetter(5), doc='Alias for field number 5')\n",
      "\n",
      "    dep_delay = _property(_itemgetter(6), doc='Alias for field number 6')\n",
      "\n",
      "    arv = _property(_itemgetter(7), doc='Alias for field number 7')\n",
      "\n",
      "    arv_delay = _property(_itemgetter(8), doc='Alias for field number 8')\n",
      "\n",
      "    airtime = _property(_itemgetter(9), doc='Alias for field number 9')\n",
      "\n",
      "    distance = _property(_itemgetter(10), doc='Alias for field number 10')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fields = ('date','airline','flightnum','origin','dest','dep','dep_delay','arv','arv_delay','airtime','distance')\n",
    "\n",
    "Flight = namedtuple('Flight',fields,verbose=True)\n",
    "\n",
    "DATE_FMT = \"%Y-%m-%d\"\n",
    "TIME_FMT = \"%H%M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(row):\n",
    "    row[0] = datetime.strptime(row[0], DATE_FMT).date()\n",
    "    row[5] = datetime.strptime(row[5], TIME_FMT).time()\n",
    "    row[6] = float(row[6])\n",
    "    row[7] = datetime.strptime(row[7], TIME_FMT).time()\n",
    "    row[8] = float(row[8])\n",
    "    row[9] = float(row[9])\n",
    "    row[10]= float(row[10])\n",
    "    return Flight(*row[:11])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flightsParsed = flights.map(lambda x: x.split(',')).map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flight(date=datetime.date(2014, 4, 1), airline='19805', flightnum='1', origin='JFK', dest='LAX', dep=datetime.time(8, 54), dep_delay=-6.0, arv=datetime.time(12, 17), arv_delay=2.0, airtime=355.0, distance=2475.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsParsed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[41] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsParsed.map(lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794.8585013871385\n"
     ]
    }
   ],
   "source": [
    "totalDistance=flightsParsed.map(lambda x:x.distance).reduce(lambda x,y:x+y)\n",
    "avgDistance=totalDistance/flightsParsed.count()\n",
    "print (avgDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3753871510922012"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsParsed.filter(lambda x:x.dep_delay>0).count()/float(flightsParsed.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[47] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsParsed.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3964730.0, 476881)\n"
     ]
    }
   ],
   "source": [
    "sumCount=flightsParsed.map(lambda x:x.dep_delay).aggregate((0,0),\n",
    "                                                           (lambda some, value: (some[0] + value, some[1]+1)),\n",
    "                                                           (lambda some1,some2: (some1[0]+some2[0],some1[1]+some2[1])))\n",
    "\n",
    "\n",
    "print (sumCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.313877046894298"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCount[0]/float(sumCount[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 452963,\n",
       "             1: 16016,\n",
       "             2: 4893,\n",
       "             3: 1729,\n",
       "             4: 701,\n",
       "             5: 249,\n",
       "             6: 113,\n",
       "             7: 66,\n",
       "             8: 43,\n",
       "             9: 26,\n",
       "             10: 15,\n",
       "             11: 12,\n",
       "             12: 9,\n",
       "             13: 15,\n",
       "             14: 13,\n",
       "             15: 4,\n",
       "             17: 2,\n",
       "             20: 4,\n",
       "             21: 3,\n",
       "             24: 3,\n",
       "             25: 1,\n",
       "             28: 1})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsParsed.map(lambda x: int(x.dep_delay/60)).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
